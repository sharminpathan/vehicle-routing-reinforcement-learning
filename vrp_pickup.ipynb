{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import sgd\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "import json\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, LSTM, Masking, MaxPooling1D, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(object):\n",
    "    def __init__(self):\n",
    "        self.n_cust=3\n",
    "        self.depot=[]\n",
    "        self.capacity = 15               #vehicle capacity\n",
    "        self.input_dim = 3              #x,y,demand\n",
    "        self.input_data = np.zeros((self.n_cust,self.input_dim))\n",
    "        self.demand = self.input_data[:,-1]\n",
    "        self.path=[self.depot]\n",
    "        self.reward=0\n",
    "        self.was_zero=False\n",
    "        self.total_reward=0\n",
    "        \n",
    "    def create_dataset(self):\n",
    "        rnd=np.random\n",
    "        x = rnd.uniform(0,1,size=(self.n_cust,2))\n",
    "        d = rnd.randint(1,10,[self.n_cust,1])\n",
    "        self.depot=[rnd.uniform(0,1),rnd.uniform(0,1)]\n",
    "        self.input_data = np.concatenate([x,d],1)\n",
    "        print(self.depot)\n",
    "        print(self.input_data)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.create_dataset()\n",
    "        self.path=[self.depot]\n",
    "        self.reward=0\n",
    "        \n",
    "    def update_state(self,cust_id):\n",
    "        if self.input_data[cust_id,-1]==0:\n",
    "            was_zero=True\n",
    "            return\n",
    "        if self.capacity>self.input_data[cust_id,-1]:\n",
    "            self.input_data[cust_id,-1]=0\n",
    "            self.capacity=self.capacity-self.input_data[cust_id,-1]\n",
    "        else:\n",
    "            self.input_data[cust_id,-1]=self.input_data[cust_id,-1]-self.capacity\n",
    "            self.capacity=max(0,self.capacity-self.input_data[cust_id,-1])\n",
    "            \n",
    "    def is_over(self):\n",
    "        return not self.input_data[:,-1].any()\n",
    "    \n",
    "    def observe(self):\n",
    "        return self.input_data.copy(), self.path.copy()\n",
    "    \n",
    "    def act(self, cust_id):\n",
    "        self.update_state(action)\n",
    "        self.path.append(self.input_data[cust_id,:-1].tolist())\n",
    "        reward = self.get_reward(cust_id)\n",
    "        if self.capacity==0 and self.input_data[:,-1].any():\n",
    "            self.path.append([depot])\n",
    "            reward = self.get_reward(cust_id)\n",
    "        game_over = self.is_over()\n",
    "        return self.input_data.copy(), self.path.copy(), reward, game_over\n",
    "    \n",
    "    def get_reward(self, cust_id):\n",
    "        \n",
    "        if self.was_zero:\n",
    "            self.was_zero=False\n",
    "            return -100\n",
    "        \n",
    "        dist = float(math.sqrt( ((self.path[-2][0]-self.path[-1][0])**2) + ((self.path[-2][1]-self.path[-1][1])**2) ))\n",
    "        if self.is_over():\n",
    "            dist=float(dist+math.sqrt( ((self.path[-2][0]-self.path[-1][0])**2) + ((self.path[-2][1]-self.path[-1][1])**2) ))\n",
    "        \n",
    "        return -dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, discount=.9): \n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "        \n",
    "    def remember(self, states, game_over):\n",
    "        self.memory.append([states, game_over])\n",
    "            \n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "        env_dim = self.memory[0][0][0]\n",
    "        path_dim = self.memory[0][0][1]\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim.shape[0],env_dim.shape[1]))\n",
    "        paths=[]\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=inputs.shape[0])):\n",
    "            input_state_t, path_state_t, action_t, reward_t, input_state_tp1, path_state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "            inputs[i]=input_state_t\n",
    "            paths.append(path_state_t)\n",
    "            targets[i]=(model.predict([np.expand_dims(input_t, axis=0), np.expand_dims(path_t, axis=1)])[0])\n",
    "            Q_sa = np.max(model.predict([np.expand_dims(input_state_tp1, axis=0), np.expand_dims(path_state_tp1, axis=1)]))\n",
    "            \n",
    "            if game_over:  # if game_over is True\n",
    "                targets[i,action_t] = reward_t\n",
    "                print(\"reward_when_over:\",reward_t)\n",
    "            else:\n",
    "                targets[i,action_t] = reward_t + self.discount * Q_sa\n",
    "         \n",
    "        return inputs, paths, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded(paths):\n",
    "    length=1\n",
    "    for p in range(len(paths)):\n",
    "        if paths[p].shape[0]>length: length=paths[p].shape[0]\n",
    "\n",
    "    \n",
    "    for p in range(len(paths)):\n",
    "        for l in range(length-paths[p].shape[0]):\n",
    "            paths[p]=(np.append(paths[p],[[0,0]],axis=0))\n",
    "            \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # parameters\n",
    "    epsilon = .1  # exploration\n",
    "    num_actions = 3  #n_cust\n",
    "    epoch = 5000\n",
    "    #max_memory = 500\n",
    "    hidden_size = 100\n",
    "    batch_size = 50\n",
    "    mem=[]\n",
    "    env=Env()\n",
    "    num_actions = env.n_cust #we need idx\n",
    "    m1=mem.copy()\n",
    "    total_reward=0\n",
    "    \n",
    "    actor_in=Input(shape=(3,3)) \n",
    "    actor=Conv1D(128,1) (actor_in)\n",
    "    actor=Flatten() (actor)\n",
    "    actor_c=Dense(128) (actor)\n",
    "    \n",
    "    decoder_in=Input(shape=(None,2))\n",
    "    decoder=Masking(mask_value=[0,0]) (decoder_in)\n",
    "    decoder_out=LSTM(128, dropout=0.1) (decoder)\n",
    "    \n",
    "    actor=concatenate([actor_c, decoder_out])\n",
    "    actor=Activation('tanh') (actor)\n",
    "    actor_out=Dense(num_actions, activation='softmax') (actor)\n",
    "    \n",
    "    critic_in=concatenate([actor_c, actor_out])\n",
    "    critic=concatenate([actor_c, critic_in])\n",
    "    critic=Activation('tanh') (critic)\n",
    "    critic_out=Dense(num_actions, activation='softmax') (critic)\n",
    "    \n",
    "    model = keras.models.Model(input=[actor_in,decoder_in], output=critic_out)\n",
    "    model.compile(sgd(lr=0.0001), loss=\"mse\")\n",
    "\n",
    "    \n",
    "    exp=Experience()\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        loss = 0.\n",
    "        env.reset()\n",
    "        game_over = False\n",
    "        # get initial input\n",
    "        input_t, path_t = env.observe()\n",
    "\n",
    "        while not game_over:\n",
    "            input_tm1 = np.array(input_t)\n",
    "            path_tm1 = np.array(path_t)\n",
    "            # get next action\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, num_actions)\n",
    "            else:\n",
    "                q = model.predict([np.expand_dims(input_tm1, axis=0), np.expand_dims(path_tm1, axis=1)])\n",
    "                action = np.argmax(q[0])\n",
    "                \n",
    "            # apply action, get rewards and new state\n",
    "            input_t, path_t, reward, game_over = env.act(action)\n",
    "            total_reward=total_reward+reward\n",
    "            \n",
    "            # store experience\n",
    "            exp.remember([input_tm1, path_tm1, action, reward, input_t, path_t], game_over)\n",
    "            \n",
    "            # adapt model\n",
    "            inputs, paths, targets = exp.get_batch(model, batch_size=batch_size)\n",
    "             \n",
    "            paths=padded(paths)\n",
    "             \n",
    "            loss += model.train_on_batch([inputs, np.array(paths)], targets)\n",
    "            \n",
    "        print(\"Epoch {:03d}/999 | Loss {:.4f} | Path_length {}\".format(e, loss, total_reward))\n",
    "        total_reward=0\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    model.save_weights(\"model.h5\", overwrite=True)\n",
    "    with open(\"model.json\", \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
